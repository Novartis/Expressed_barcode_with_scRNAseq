{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scvelo as scv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scAnalysis as scrna\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "scv.__version__\n",
    "scv.settings.presenter_view = True  # set max width size for presenter view\n",
    "scv.settings.set_figure_params('scvelo')  # for beautified visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(dfin,features,Npca=10,number_genes=[],zscore=True):\n",
    "    #number_genes is the number of genes used to compute PCA, ordered bu std\n",
    "    \n",
    "    \n",
    "    \n",
    "    if number_genes==[]:\n",
    "        features2=features\n",
    "    else:\n",
    "        number_genes=min(number_genes,len(features))\n",
    "        df=dfin.copy()\n",
    "        df.loc['stds',:]=[0]*len(df.columns)\n",
    "        df.loc['stds',features] = df.loc[:,features].std(axis=0,ddof=0)/df.loc[:,features].mean(axis=0)\n",
    "        df=df.sort_values('stds',axis=1,ascending=False)\n",
    "        features2 = df.columns[0:number_genes]\n",
    "        \n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "\n",
    "    \n",
    "    #Compte actual PCs using all samples\n",
    "    print('Computing PCs...')\n",
    "    X = dfin.loc[:,features2].dropna()\n",
    "    if zscore:\n",
    "        for col in X.columns:\n",
    "            X[col] = (X[col] - X[col].mean())/X[col].std(ddof=0)\n",
    "    X=X.fillna(0)\n",
    "    pca = PCA(n_components=Npca)\n",
    "    PCs = pca.fit_transform(X)\n",
    "    perc = 100*pca.explained_variance_ratio_.sum()\n",
    "    print('Done!')\n",
    "\n",
    "    print('We use '+str(Npca)+' components to explain '+str(perc)+'% of the variability')\n",
    "\n",
    "    #create DF with PCA results \n",
    "    dfout = dfin.copy()\n",
    "    pclist = []\n",
    "    for i in range(PCs.shape[1]):\n",
    "        dfout.loc[:,'PC'+str(i+1)] = PCs[:,i]\n",
    "        pclist.append('PC'+str(i+1))\n",
    "\n",
    "    print(pca.explained_variance_ratio_)  \n",
    "    print(pca.explained_variance_ratio_.sum())\n",
    "    \n",
    "    loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n",
    "    loads = pd.DataFrame(loadings,index=features2,columns = pclist)\n",
    "    loads =loads.sort_values('PC1',ascending=False)\n",
    "    \n",
    "    return dfout, loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATA\n",
    "results_file = 'PC9.h5ad'\n",
    "#results_file = 'HCC827velo_nolayer.h5ad'#'PC9.h5ad'#\n",
    "#results_file = 'HCC4006.h5ad'\n",
    "adata = sc.read('/home/estraja4/BFX_research/Resistance/barcoding/final_anndatasNEW/'+results_file)\n",
    "celltype='PC9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrna.plot_label(adata, feat='UMAP', stratify='louvain', legend_inside=True, figsize=(9, 7), ax='',savepdf=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrna.plot_label(adata, feat='UMAP', stratify='timepoint', legend_inside=False, figsize=(9, 7), ax='',savepdf=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_kmeans(df,n_clusters=4):\n",
    "\n",
    "    import sklearn.cluster as cluster\n",
    "    import seaborn as sns\n",
    "\n",
    "    features=[i for i in df.columns if 'PC' in i]\n",
    "    X = df.loc[:,features]\n",
    "\n",
    "    clustering = cluster.KMeans(n_clusters=n_clusters)#.AffinityPropagation()#n_clusters=n_clusters)#\n",
    "    clustering.fit(X)\n",
    "\n",
    "    df2=df.copy()\n",
    "    df2['clustering']=[str(i) for i in clustering.labels_]\n",
    "    \n",
    "    #sns.lmplot(data=df2,x='PC1',y='PC2',hue='clustering',fit_reg=False)\n",
    "    #for i in range(len(df)):\n",
    "    #    plt.text(df2.iloc[i]['PC1'],df2.iloc[i]['PC2'],str(i))\n",
    "    #plt.show()\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transitions based on cloneid\n",
    "For each cloneid, compute its proportion in each of the louvain clusters. Use that information to compute likely transitions between timepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding='pca'\n",
    "\n",
    "columns=[t for t in adata.obs['timepoint'].unique()]\n",
    "for t in adata.obs['timepoint'].unique():\n",
    "    for j in range(adata.obsm['X_'+embedding].shape[1]):\n",
    "        columns.append(t+'_'+str(j))\n",
    "    columns.append(t+'_count')\n",
    "        \n",
    "transitions=pd.DataFrame(index=[bc for bc in adata.obs['cloneid'].unique() if 'no-bc' not in bc and '-' not in bc],columns=columns)\n",
    "for bc in adata.obs['cloneid'].unique():\n",
    "    if 'no-bc' not in bc and '-' not in bc:\n",
    "        aux=adata[adata.obs['cloneid']==bc]\n",
    "        for t in adata.obs['timepoint'].unique():\n",
    "            aux2=aux[aux.obs['timepoint']==t]\n",
    "            val=np.median(aux2.obsm['X_'+embedding],axis=0)\n",
    "            val=[i for i in val]\n",
    "            transitions[t].loc[bc]=val\n",
    "            for j in range(aux2.obsm['X_'+embedding].shape[1]):\n",
    "                transitions[t+'_'+str(j)].loc[bc]=val[j]\n",
    "            transitions[t+'_count'].loc[bc]=len(aux2)\n",
    "transitions2pca=transitions.replace('nan',np.nan).dropna()\n",
    "\n",
    "feats=[t for t in transitions2pca.columns if '_' in t and 'count' not in t]\n",
    "transitions3pca,load = do_pca(transitions2pca,feats,Npca=4,number_genes=[],zscore=False)\n",
    "transitions3pca=add_kmeans(transitions3pca,n_clusters=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding='umap'\n",
    "\n",
    "columns=[t for t in adata.obs['timepoint'].unique()]\n",
    "for t in adata.obs['timepoint'].unique():\n",
    "    for j in range(adata.obsm['X_'+embedding].shape[1]):\n",
    "        columns.append(t+'_'+str(j))\n",
    "    columns.append(t+'_count')\n",
    "        \n",
    "transitions=pd.DataFrame(index=[bc for bc in adata.obs['cloneid'].unique() if 'no-bc' not in bc and '-' not in bc],columns=columns)\n",
    "for bc in adata.obs['cloneid'].unique():\n",
    "    if 'no-bc' not in bc and '-' not in bc:\n",
    "        aux=adata[adata.obs['cloneid']==bc]\n",
    "        for t in adata.obs['timepoint'].unique():\n",
    "            aux2=aux[aux.obs['timepoint']==t]\n",
    "            val=np.median(aux2.obsm['X_'+embedding],axis=0)\n",
    "            val=[i for i in val]\n",
    "            transitions[t].loc[bc]=val\n",
    "            for j in range(aux2.obsm['X_'+embedding].shape[1]):\n",
    "                transitions[t+'_'+str(j)].loc[bc]=val[j]\n",
    "            transitions[t+'_count'].loc[bc]=len(aux2)\n",
    "transitions2umap=transitions.replace('nan',np.nan).dropna()\n",
    "\n",
    "feats=[t for t in transitions2umap.columns if '_' in t and 'count' not in t]\n",
    "transitions3umap,load = do_pca(transitions2umap,feats,Npca=4,number_genes=[],zscore=True)\n",
    "#transitions3umap=add_kmeans(transitions3umap,n_clusters=5)\n",
    "\n",
    "#read clustering from andata\n",
    "df=scrna.andata2df(adata)\n",
    "df=df[['cloneid','trajectory_class']]\n",
    "mask1=~np.isin(df['trajectory_class'],['-'])\n",
    "mask2=['earlier' not in i and 'later' not in i for i in df['trajectory_class']]\n",
    "df=df.loc[mask1&mask2]\n",
    "\n",
    "cid=[]\n",
    "cl=[]\n",
    "for c in df['cloneid'].unique():\n",
    "    cid.append(c)\n",
    "    cl.append(df.loc[df['cloneid']==c]['trajectory_class'][0])\n",
    "df2=pd.DataFrame(index=cid)\n",
    "df2['clustering']=cl\n",
    "    \n",
    "transitions3umap=transitions3umap.merge(df2,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "umap=transitions3umap[[i for i in transitions3umap.columns if 'PC' in i]+['clustering']]\n",
    "umap=umap.rename(columns={i:i+'_UMAP' for i in umap.columns})\n",
    "pca=transitions3pca[[i for i in transitions3pca.columns if 'PC' in i]+['clustering']]\n",
    "pca=pca.rename(columns={i:i+'_PC' for i in pca.columns})\n",
    "\n",
    "clusters = umap.merge(pca,left_index=True,right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "df=clusters\n",
    "axs=ax[0]\n",
    "hue='clustering_PC'\n",
    "df=df.sort_values(hue)\n",
    "for h in df[hue].unique():\n",
    "    df2=df.loc[df[hue]==h]\n",
    "    axs.scatter(df2['PC1_PC'],df2['PC2_PC'],marker='o',s=100,label=h)\n",
    "    axs.legend(fontsize=16)\n",
    "    axs.axes.xaxis.set_ticks([])\n",
    "    axs.axes.yaxis.set_ticks([])\n",
    "    axs.set_xlabel('PC1',fontsize=16)\n",
    "    axs.set_ylabel('PC2',fontsize=16)\n",
    "axs.set_title('PCA based, PC clusters')\n",
    "    \n",
    "axs=ax[1]\n",
    "hue='clustering_UMAP'\n",
    "df=df.sort_values(hue)\n",
    "for h in df[hue].unique():\n",
    "    df2=df.loc[df[hue]==h]\n",
    "    axs.scatter(df2['PC1_UMAP'],df2['PC2_UMAP'],marker='o',s=100,label=h)\n",
    "    axs.legend(fontsize=16)\n",
    "    axs.axes.xaxis.set_ticks([])\n",
    "    axs.axes.yaxis.set_ticks([])\n",
    "    axs.set_xlabel('PC1',fontsize=16)\n",
    "    axs.set_ylabel('PC2',fontsize=16)\n",
    "axs.set_title('UMAP based, UMAP clusters')\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "df=clusters\n",
    "axs=ax[0]\n",
    "hue='clustering_UMAP'\n",
    "df=df.sort_values(hue)\n",
    "for h in df[hue].unique():\n",
    "    df2=df.loc[df[hue]==h]\n",
    "    axs.scatter(df2['PC1_PC'],df2['PC2_PC'],marker='o',s=100,label=h)\n",
    "    axs.legend(fontsize=16)\n",
    "    axs.axes.xaxis.set_ticks([])\n",
    "    axs.axes.yaxis.set_ticks([])\n",
    "    axs.set_xlabel('PC1',fontsize=16)\n",
    "    axs.set_ylabel('PC2',fontsize=16)\n",
    "axs.set_title('PCA based, UMAP clusters')\n",
    "    \n",
    "axs=ax[1]\n",
    "hue='clustering_PC'\n",
    "df=df.sort_values(hue)\n",
    "for h in df[hue].unique():\n",
    "    df2=df.loc[df[hue]==h]\n",
    "    axs.scatter(df2['PC1_UMAP'],df2['PC2_UMAP'],marker='o',s=100,label=h)\n",
    "    axs.legend(fontsize=16)\n",
    "    axs.axes.xaxis.set_ticks([])\n",
    "    axs.axes.yaxis.set_ticks([])\n",
    "    axs.set_xlabel('PC1',fontsize=16)\n",
    "    axs.set_ylabel('PC2',fontsize=16)\n",
    "axs.set_title('UMAP based, PCA clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename clusters for comparison: THIS HAS TO BE TWEAKED EVERY RUN!\n",
    "clusters['clustering_PC']=clusters['clustering_PC'].map({'0':'3','1':'4','2':'1','3':'2','4':'0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "df=clusters\n",
    "axs=ax[0]\n",
    "hue='clustering_UMAP'\n",
    "df=df.sort_values(hue)\n",
    "for h in df[hue].unique():\n",
    "    df2=df.loc[df[hue]==h]\n",
    "    axs.scatter(df2['PC1_UMAP'],df2['PC2_UMAP'],marker='o',s=100,label=h)\n",
    "    axs.legend(fontsize=16)\n",
    "    axs.axes.xaxis.set_ticks([])\n",
    "    axs.axes.yaxis.set_ticks([])\n",
    "    axs.set_xlabel('PC1',fontsize=16)\n",
    "    axs.set_ylabel('PC2',fontsize=16)\n",
    "axs.set_title('UMAP clusters',fontsize=14)\n",
    "\n",
    "    \n",
    "axs=ax[1]\n",
    "hue='clustering_PC'\n",
    "df=df.sort_values(hue)\n",
    "for h in df[hue].unique():\n",
    "    df2=df.loc[df[hue]==h]\n",
    "    axs.scatter(df2['PC1_UMAP'],df2['PC2_UMAP'],marker='o',s=100,label=h)\n",
    "    axs.legend(fontsize=16)\n",
    "    axs.axes.xaxis.set_ticks([])\n",
    "    axs.axes.yaxis.set_ticks([])\n",
    "    axs.set_xlabel('PC1',fontsize=16)\n",
    "    axs.set_ylabel('PC2',fontsize=16)\n",
    "axs.set_title('PCA clusters',fontsize=14)\n",
    "plt.show()\n",
    "fig.savefig(\"For_referees/clustersPCvsUAMP_MGHPC9.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clusters.to_csv('For_referees/clustersPCvsUAMP_HCC4006.csv')\n",
    "clusters.to_csv('For_referees/clustersPCvsUAMP_PC9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from matplotlib_venn import venn2, venn2_circles, venn2_unweighted\n",
    "from matplotlib_venn import venn3, venn3_circles\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 5, figsize=(20, 5))\n",
    "k=0\n",
    "for c in ['0','1','2','3','4']:\n",
    "    ax=axs[k]\n",
    "    maskUMAP=clusters['clustering_UMAP']==c\n",
    "    maskPC=clusters['clustering_PC']==c\n",
    "    a = len(clusters.loc[maskUMAP&~maskPC])\n",
    "    b = len(clusters.loc[maskPC&~maskUMAP])\n",
    "    ab = len(clusters.loc[maskPC&maskUMAP])\n",
    "    k=k+1\n",
    "\n",
    "    venn2(subsets = (a, b, ab), set_labels = ('UMAP', 'PCA'),ax=ax)\n",
    "    ax.set_title('trajectory class '+c)\n",
    "plt.show()\n",
    "fig.savefig(\"For_referees/vennPCvsUAMP_PC9.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PLENTY.v2022.1",
   "language": "python",
   "name": "plenty.v2022.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
